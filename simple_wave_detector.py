import cv2
import os
import numpy as np
from ultralytics import YOLO
from datetime import datetime
from collections import deque, defaultdict
import math

class SimplePersonTracker:
    """ç®€åŒ–çš„äººç‰©è·Ÿè¸ªå™¨"""
    
    def __init__(self, max_distance=100, max_frames_lost=10):
        self.tracks = {}
        self.next_id = 0
        self.max_distance = max_distance
        self.max_frames_lost = max_frames_lost
        
    def update(self, detections):
        detection_centers = []
        for det in detections:
            x1, y1, x2, y2, conf = det
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2
            detection_centers.append((center_x, center_y, det))
        
        # æ›´æ–°ç°æœ‰è½¨è¿¹
        for track_id in list(self.tracks.keys()):
            self.tracks[track_id]['frames_lost'] += 1
            if self.tracks[track_id]['frames_lost'] > self.max_frames_lost:
                del self.tracks[track_id]
        
        # åŒ¹é…æ£€æµ‹ç»“æœ
        matched_tracks = set()
        matched_detections = set()
        
        for i, (det_x, det_y, det) in enumerate(detection_centers):
            best_match = None
            best_distance = float('inf')
            
            for track_id, track_info in self.tracks.items():
                if track_id in matched_tracks:
                    continue
                    
                last_x, last_y = track_info['last_center']
                distance = math.sqrt((det_x - last_x)**2 + (det_y - last_y)**2)
                
                if distance < self.max_distance and distance < best_distance:
                    best_distance = distance
                    best_match = track_id
            
            if best_match is not None:
                self.tracks[best_match]['last_center'] = (det_x, det_y)
                self.tracks[best_match]['last_bbox'] = det
                self.tracks[best_match]['frames_lost'] = 0
                matched_tracks.add(best_match)
                matched_detections.add(i)
        
        # åˆ›å»ºæ–°è½¨è¿¹
        for i, (det_x, det_y, det) in enumerate(detection_centers):
            if i not in matched_detections:
                self.tracks[self.next_id] = {
                    'last_center': (det_x, det_y),
                    'last_bbox': det,
                    'frames_lost': 0
                }
                self.next_id += 1
        
        return self.tracks

class SimpleWaveDetector:
    """åŸºäºå®½é«˜æ¯”çš„ç®€åŒ–æŒ¥æ‰‹æ£€æµ‹å™¨"""
    
    def __init__(self, 
                 normal_aspect_ratio_max=0.6,  # æ­£å¸¸äººä½“å®½é«˜æ¯”ä¸Šé™
                 wave_aspect_ratio_min=0.7,    # æŒ¥æ‰‹æ—¶å®½é«˜æ¯”ä¸‹é™
                 history_length=10,             # å†å²å¸§æ•°
                 min_wave_duration=3):          # æœ€å°æŒ¥æ‰‹æŒç»­å¸§æ•°
        
        self.normal_aspect_ratio_max = normal_aspect_ratio_max
        self.wave_aspect_ratio_min = wave_aspect_ratio_min
        self.history_length = history_length
        self.min_wave_duration = min_wave_duration
        
        self.person_histories = defaultdict(lambda: deque(maxlen=history_length))
        self.wave_counters = defaultdict(int)  # è¿ç»­æŒ¥æ‰‹å¸§è®¡æ•°
        
    def detect_wave_simple(self, track_id, bbox):
        """ç®€åŒ–çš„æŒ¥æ‰‹æ£€æµ‹ï¼šä¸»è¦åŸºäºå®½é«˜æ¯”"""
        x1, y1, x2, y2, conf = bbox
        width = x2 - x1
        height = y2 - y1
        
        if height <= 0:
            return False, {'aspect_ratio': 0, 'is_waving': False, 'reason': 'invalid_height'}
        
        aspect_ratio = width / height
        
        # è®°å½•å†å²
        self.person_histories[track_id].append(aspect_ratio)
        
        # åˆ¤æ–­é€»è¾‘
        is_current_frame_wave = aspect_ratio >= self.wave_aspect_ratio_min
        
        if is_current_frame_wave:
            self.wave_counters[track_id] += 1
        else:
            self.wave_counters[track_id] = 0  # é‡ç½®è®¡æ•°å™¨
        
        # éœ€è¦è¿ç»­å‡ å¸§éƒ½æ˜¯æŒ¥æ‰‹çŠ¶æ€æ‰è®¤ä¸ºæ˜¯çœŸæ­£çš„æŒ¥æ‰‹
        is_sustained_wave = self.wave_counters[track_id] >= self.min_wave_duration
        
        # è·å–å†å²ç»Ÿè®¡
        history = list(self.person_histories[track_id])
        avg_aspect_ratio = np.mean(history) if history else aspect_ratio
        max_aspect_ratio = max(history) if history else aspect_ratio
        
        debug_info = {
            'aspect_ratio': aspect_ratio,
            'avg_aspect_ratio': avg_aspect_ratio,
            'max_aspect_ratio': max_aspect_ratio,
            'wave_counter': self.wave_counters[track_id],
            'is_current_wave': is_current_frame_wave,
            'is_sustained_wave': is_sustained_wave,
            'width': width,
            'height': height
        }
        
        return is_sustained_wave, debug_info

def analyze_video_simple_wave_detection(video_path, output_dir=None, show_video=True, save_output=True):
    """ä½¿ç”¨ç®€åŒ–ç®—æ³•è¿›è¡ŒæŒ¥æ‰‹æ£€æµ‹"""
    print(f"=== ç®€åŒ–ç‰ˆæŒ¥æ‰‹æ£€æµ‹ ===")
    print(f"æ£€æµ‹åŸç†ï¼šäººç‰©å®½é«˜æ¯” >= 0.7 ä¸”æŒç»­3å¸§ä»¥ä¸Š = æŒ¥æ‰‹")
    print(f"è§†é¢‘æ–‡ä»¶: {video_path}")
    
    if not os.path.exists(video_path):
        print(f"é”™è¯¯ï¼šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    # åŠ è½½YOLOäººç‰©æ£€æµ‹æ¨¡å‹
    model = YOLO('yolov8n.pt')
    
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"é”™è¯¯ï¼šæ— æ³•æ‰“å¼€è§†é¢‘")
        return False
    
    # è·å–è§†é¢‘å±æ€§
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"è§†é¢‘å±æ€§: {width}x{height}, {fps:.1f}fps, {total_frames}å¸§")
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    tracker = SimplePersonTracker()
    wave_detector = SimpleWaveDetector(
        normal_aspect_ratio_max=0.6,
        wave_aspect_ratio_min=0.7,   # å¯ä»¥è°ƒæ•´è¿™ä¸ªé˜ˆå€¼
        min_wave_duration=3
    )
    
    # å‡†å¤‡è¾“å‡ºè§†é¢‘
    output_writer = None
    if save_output and output_dir:
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        
        # æ ¹æ®åŸå§‹æ–‡ä»¶æ ¼å¼é€‰æ‹©è¾“å‡ºæ ¼å¼
        original_ext = os.path.splitext(video_path)[1].lower()
        if original_ext == '.mov':
            output_path = os.path.join(output_dir, f"{video_name}_simple_wave_{timestamp}.mov")
            fourcc = cv2.VideoWriter.fourcc(*'mp4v')
        else:
            output_path = os.path.join(output_dir, f"{video_name}_simple_wave_{timestamp}.avi")
            fourcc = cv2.VideoWriter.fourcc(*'XVID')
        
        # ç¡®ä¿å¸§ç‡æœ‰æ•ˆ
        if fps <= 0 or fps > 60:
            fps = 25.0
            
        output_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        if output_writer and output_writer.isOpened():
            print(f"ä¿å­˜ç»“æœåˆ°: {output_path}")
        else:
            print(f"è­¦å‘Šï¼šæ— æ³•åˆ›å»ºè¾“å‡ºè§†é¢‘æ–‡ä»¶")
            output_writer = None
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_frames': 0,
        'frames_with_persons': 0,
        'frames_with_waves': 0,
        'total_wave_detections': 0,
        'high_aspect_ratio_frames': 0  # å®½é«˜æ¯”>0.7çš„å¸§æ•°
    }
    
    frame_count = 0
    
    print(f"\nå¼€å§‹åˆ†æ... (æŒ‰ 'q' é€€å‡º)")
    print(f"æ£€æµ‹é˜ˆå€¼: å®½é«˜æ¯” >= {wave_detector.wave_aspect_ratio_min}")
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            stats['total_frames'] = frame_count
            
            # ä½¿ç”¨YOLOæ£€æµ‹äººç‰©
            results = model(frame, conf=0.5, verbose=False)[0]
            boxes = results.boxes
            
            # æå–äººç‰©æ£€æµ‹ç»“æœ
            person_detections = []
            if boxes is not None and len(boxes) > 0:
                for box in boxes:
                    cls_id = int(box.cls[0])
                    if cls_id == 0:  # äººç±»
                        conf_score = float(box.conf[0])
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        person_detections.append((x1, y1, x2, y2, conf_score))
            
            if person_detections:
                stats['frames_with_persons'] += 1
            
            # æ›´æ–°äººç‰©è·Ÿè¸ª
            tracks = tracker.update(person_detections)
            
            # æ£€æµ‹æŒ¥æ‰‹åŠ¨ä½œ
            current_frame_has_wave = False
            wave_count = 0
            
            for track_id, track_info in tracks.items():
                if track_info['frames_lost'] == 0:  # å½“å‰å¸§æœ‰æ£€æµ‹ç»“æœ
                    bbox = track_info['last_bbox']
                    is_waving, wave_info = wave_detector.detect_wave_simple(track_id, bbox)
                    
                    x1, y1, x2, y2, conf = bbox
                    aspect_ratio = wave_info['aspect_ratio']
                    
                    # ç»Ÿè®¡é«˜å®½é«˜æ¯”å¸§
                    if aspect_ratio >= 0.7:
                        stats['high_aspect_ratio_frames'] += 1
                    
                    # ç»˜åˆ¶è¾¹ç•Œæ¡†
                    if is_waving:
                        color = (0, 255, 0)  # ç»¿è‰² - æŒ¥æ‰‹
                        label = f"Person {track_id}: WAVING"
                        current_frame_has_wave = True
                        wave_count += 1
                        stats['total_wave_detections'] += 1
                    elif wave_info['is_current_wave']:
                        color = (0, 255, 255)  # é»„è‰² - å¯èƒ½æŒ¥æ‰‹ï¼ˆæŒç»­æ—¶é—´ä¸å¤Ÿï¼‰
                        label = f"Person {track_id}: MAYBE WAVE"
                    else:
                        color = (255, 0, 0)  # è“è‰² - æ­£å¸¸
                        label = f"Person {track_id}: Normal"
                    
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                    cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                    
                    # æ˜¾ç¤ºå®½é«˜æ¯”ä¿¡æ¯
                    ratio_text = f"Ratio: {aspect_ratio:.2f} ({wave_info['wave_counter']})"
                    cv2.putText(frame, ratio_text, (x1, y2+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            if current_frame_has_wave:
                stats['frames_with_waves'] += 1
            
            # åœ¨ç”»é¢ä¸Šæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            info_text = f"Frame: {frame_count}/{total_frames} | Persons: {len([t for t in tracks.values() if t['frames_lost'] == 0])} | Waving: {wave_count}"
            cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # æ˜¾ç¤ºæ£€æµ‹é˜ˆå€¼
            threshold_text = f"Wave Threshold: Aspect Ratio >= {wave_detector.wave_aspect_ratio_min}"
            cv2.putText(frame, threshold_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # ä¿å­˜è¾“å‡ºå¸§
            if output_writer:
                output_writer.write(frame)
            
            # æ˜¾ç¤ºè§†é¢‘
            if show_video:
                cv2.imshow('Simple Wave Detection', frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            
            # è¿›åº¦æŠ¥å‘Š
            if frame_count % 100 == 0:
                progress = (frame_count / total_frames) * 100
                print(f"è¿›åº¦: {progress:.1f}% - æŒ¥æ‰‹: {stats['total_wave_detections']}, é«˜å®½é«˜æ¯”: {stats['high_aspect_ratio_frames']}")
    
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­åˆ†æ")
    
    # æ¸…ç†èµ„æº
    cap.release()
    if output_writer:
        output_writer.release()
    if show_video:
        cv2.destroyAllWindows()
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\n=== ç®€åŒ–ç®—æ³•åˆ†æç»“æœ ===")
    print(f"æ€»å¸§æ•°: {stats['total_frames']}")
    print(f"æœ‰äººç‰©çš„å¸§æ•°: {stats['frames_with_persons']}")
    print(f"æ£€æµ‹åˆ°æŒ¥æ‰‹çš„å¸§æ•°: {stats['frames_with_waves']}")
    print(f"æ€»æŒ¥æ‰‹æ£€æµ‹æ¬¡æ•°: {stats['total_wave_detections']}")
    print(f"é«˜å®½é«˜æ¯”(>=0.7)å¸§æ•°: {stats['high_aspect_ratio_frames']}")
    
    if stats['frames_with_persons'] > 0:
        wave_rate = (stats['frames_with_waves'] / stats['frames_with_persons']) * 100
        high_aspect_rate = (stats['high_aspect_ratio_frames'] / stats['frames_with_persons']) * 100
        print(f"æŒ¥æ‰‹æ£€æµ‹ç‡: {wave_rate:.1f}%")
        print(f"é«˜å®½é«˜æ¯”å‡ºç°ç‡: {high_aspect_rate:.1f}%")
        
        if stats['high_aspect_ratio_frames'] > stats['total_wave_detections']:
            print(f"\nğŸ’¡ å‘ç°: æœ‰ {stats['high_aspect_ratio_frames'] - stats['total_wave_detections']} ä¸ªé«˜å®½é«˜æ¯”å¸§æœªè¢«è¯†åˆ«ä¸ºæŒ¥æ‰‹")
            print("å»ºè®®: å¯ä»¥é™ä½ min_wave_duration å‚æ•°æˆ–è°ƒæ•´ wave_aspect_ratio_min é˜ˆå€¼")
    
    if output_writer and save_output:
        print(f"\nâœ… åˆ†æè§†é¢‘å·²ä¿å­˜: {output_path}")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    video_path = r"C:\c\wave_hands\test_videos_20250622_100947\B605_3min_20250622_100957.mov"
    
    print("=== ç®€åŒ–ç‰ˆæŒ¥æ‰‹æ£€æµ‹ç®—æ³• ===")
    print("æ ¸å¿ƒæ€è·¯: å¦‚æœäººç‰©å®½é«˜æ¯” >= 0.7 ä¸”æŒç»­3å¸§ä»¥ä¸Šï¼Œåˆ™åˆ¤å®šä¸ºæŒ¥æ‰‹")
    print("è¿™ä¸ªæ–¹æ³•ç›´æ¥åˆ©ç”¨äº†æ‚¨è§‚å¯Ÿåˆ°çš„'æŒ¥æ‰‹æ—¶å®½é«˜æ¯”æ˜æ˜¾å¼‚å¸¸'ç‰¹å¾")
    
    if not os.path.exists(video_path):
        print(f"é”™è¯¯ï¼šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return
    
    # å¼€å§‹åˆ†æ
    success = analyze_video_simple_wave_detection(
        video_path=video_path,
        output_dir="simple_wave_analysis",
        show_video=True,
        save_output=True
    )
    
    if success:
        print("\nğŸ‰ ç®€åŒ–ç®—æ³•æµ‹è¯•å®Œæˆï¼")
        print("\nå¯è°ƒæ•´çš„å‚æ•°ï¼š")
        print("- wave_aspect_ratio_min: å½“å‰0.7ï¼Œå¯ä»¥è°ƒæ•´ä¸º0.6æˆ–0.8")
        print("- min_wave_duration: å½“å‰3å¸§ï¼Œå¯ä»¥è°ƒæ•´ä¸º1æˆ–2")
        print("- normal_aspect_ratio_max: å½“å‰0.6ï¼Œç”¨äºå¯¹æ¯”")

if __name__ == '__main__':
    main() 