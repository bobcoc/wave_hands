import cv2
import yaml
import os
import sys
import time
import numpy as np
from ultralytics import YOLO
from datetime import datetime
import matplotlib.pyplot as plt

def load_config(config_path='config.yaml'):
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def debug_model_detection(video_path, config):
    """
    è¯¦ç»†è°ƒè¯•æ¨¡å‹æ£€æµ‹æ•ˆæœ
    """
    print("=== æ¨¡å‹æ£€æµ‹è°ƒè¯•ç¨‹åº ===")
    
    # åŠ è½½é…ç½®å‚æ•°
    weights = config.get('weights', 'weight/best.pt')
    names = config.get('names', ['wave', 'nowave'])
    device = config.get('device', 'cpu')
    
    print(f"æ¨¡å‹æ–‡ä»¶: {weights}")
    print(f"æ£€æµ‹ç±»åˆ«: {names}")
    print(f"è®¾å¤‡: {device}")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(weights):
        print(f"é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {weights}")
        return False
    
    # åŠ è½½æ¨¡å‹
    try:
        model = YOLO(weights, task='detect')
        model.to(device)
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False
    
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"é”™è¯¯ï¼šæ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        return False
    
    # è·å–è§†é¢‘å±æ€§
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"è§†é¢‘å±æ€§: {width}x{height}, {fps:.1f}fps, {total_frames}å¸§")
    
    # æµ‹è¯•ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼
    confidence_levels = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7]
    
    print("\n=== å¤šç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯• ===")
    
    # éšæœºé€‰æ‹©å‡ å¸§è¿›è¡Œæµ‹è¯•
    test_frame_indices = np.linspace(0, total_frames-1, min(20, total_frames), dtype=int)
    test_frames = []
    
    # è¯»å–æµ‹è¯•å¸§
    for frame_idx in test_frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if ret:
            test_frames.append((frame_idx, frame.copy()))
    
    print(f"é€‰æ‹©äº† {len(test_frames)} å¸§è¿›è¡Œæµ‹è¯•")
    
    # å¯¹æ¯ä¸ªç½®ä¿¡åº¦é˜ˆå€¼è¿›è¡Œæµ‹è¯•
    results_summary = {}
    
    for conf_threshold in confidence_levels:
        print(f"\n--- æµ‹è¯•ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold} ---")
        
        total_detections = 0
        wave_count = 0
        nowave_count = 0
        frames_with_detection = 0
        
        for frame_idx, frame in test_frames:
            # è¿›è¡Œæ£€æµ‹
            results = model(frame, conf=conf_threshold, verbose=False)[0]
            boxes = results.boxes
            
            frame_detections = 0
            if boxes is not None and len(boxes) > 0:
                frames_with_detection += 1
                for box in boxes:
                    cls_id = int(box.cls[0])
                    conf_score = float(box.conf[0])
                    
                    if conf_score >= conf_threshold:
                        frame_detections += 1
                        total_detections += 1
                        
                        if cls_id == 0:  # wave
                            wave_count += 1
                        elif cls_id == 1:  # nowave
                            nowave_count += 1
            
            if frame_detections > 0:
                print(f"  å¸§ {frame_idx}: æ£€æµ‹åˆ° {frame_detections} ä¸ªç›®æ ‡")
        
        detection_rate = (frames_with_detection / len(test_frames)) * 100
        
        results_summary[conf_threshold] = {
            'total_detections': total_detections,
            'wave_count': wave_count,
            'nowave_count': nowave_count,
            'frames_with_detection': frames_with_detection,
            'detection_rate': detection_rate
        }
        
        print(f"  æ€»æ£€æµ‹æ•°: {total_detections}")
        print(f"  Wave: {wave_count}, NoWave: {nowave_count}")
        print(f"  æ£€æµ‹ç‡: {detection_rate:.1f}% ({frames_with_detection}/{len(test_frames)})")
    
    # è¾“å‡ºæ±‡æ€»ç»“æœ
    print("\n=== ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•æ±‡æ€» ===")
    print("ç½®ä¿¡åº¦\tæ€»æ£€æµ‹\tWave\tNoWave\tæ£€æµ‹ç‡")
    print("-" * 50)
    for conf, result in results_summary.items():
        print(f"{conf:.2f}\t{result['total_detections']}\t{result['wave_count']}\t{result['nowave_count']}\t{result['detection_rate']:.1f}%")
    
    # å¦‚æœæ‰€æœ‰ç½®ä¿¡åº¦ä¸‹éƒ½æ²¡æœ‰æ£€æµ‹åˆ°ç›®æ ‡ï¼Œå»ºè®®é‡æ–°è®­ç»ƒ
    max_detections = max([r['total_detections'] for r in results_summary.values()])
    
    if max_detections == 0:
        print("\nâš ï¸  æ‰€æœ‰ç½®ä¿¡åº¦é˜ˆå€¼ä¸‹éƒ½æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼")
        print("\nè¿™å¼ºçƒˆè¡¨æ˜éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå¯èƒ½çš„åŸå› ï¼š")
        print("1. è®­ç»ƒæ•°æ®ä¸å®é™…åœºæ™¯å·®å¼‚è¿‡å¤§")
        print("2. æ‘„åƒå¤´è§’åº¦ã€å…‰çº¿æ¡ä»¶ä¸è®­ç»ƒæ•°æ®ä¸åŒ¹é…")
        print("3. äººå‘˜ä½ç½®ã€æœè£…ã€èƒŒæ™¯ç­‰å› ç´ å½±å“")
        print("4. æ¨¡å‹å¯èƒ½æŸåæˆ–ä¸é€‚ç”¨")
        
        print("\nå»ºè®®çš„è§£å†³æ–¹æ¡ˆï¼š")
        print("1. ä½¿ç”¨å½“å‰æ‘„åƒå¤´å½•åˆ¶çš„è§†é¢‘é‡æ–°è®­ç»ƒæ¨¡å‹")
        print("2. æ”¶é›†ä¸åŒå…‰çº¿ã€è§’åº¦ã€äººå‘˜çš„è®­ç»ƒæ•°æ®")
        print("3. è¿›è¡Œæ•°æ®å¢å¼ºä»¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›")
        
        return False
    else:
        print(f"\nâœ“ æœ€å¤§æ£€æµ‹æ•°: {max_detections}")
        best_conf = min([conf for conf, result in results_summary.items() if result['total_detections'] == max_detections])
        print(f"âœ“ æœ€ä½³ç½®ä¿¡åº¦é˜ˆå€¼: {best_conf}")
        
        if max_detections < len(test_frames) * 0.1:  # æ£€æµ‹ç‡ä½äº10%
            print("\nâš ï¸  æ£€æµ‹ç‡è¿‡ä½ï¼Œä»å»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹")
            return False
        else:
            print(f"\nâœ“ æ¨¡å‹åœ¨ç½®ä¿¡åº¦ {best_conf} ä¸‹å·¥ä½œæ­£å¸¸")
            return True
    
    cap.release()

def visualize_sample_detections(video_path, config, num_samples=5):
    """
    å¯è§†åŒ–æ ·æœ¬æ£€æµ‹ç»“æœï¼Œä¿å­˜å›¾ç‰‡ç”¨äºåˆ†æ
    """
    print(f"\n=== ä¿å­˜æ ·æœ¬æ£€æµ‹å›¾ç‰‡ ===")
    
    weights = config.get('weights', 'weight/best.pt')
    model = YOLO(weights, task='detect')
    
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "debug_samples"
    os.makedirs(output_dir, exist_ok=True)
    
    sample_indices = np.linspace(0, total_frames-1, num_samples, dtype=int)
    
    for i, frame_idx in enumerate(sample_indices):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if not ret:
            continue
        
        # ä½¿ç”¨æä½ç½®ä¿¡åº¦è¿›è¡Œæ£€æµ‹
        results = model(frame, conf=0.01, verbose=False)[0]
        
        # ç»˜åˆ¶æ£€æµ‹ç»“æœ
        annotated_frame = frame.copy()
        boxes = results.boxes
        
        detection_count = 0
        if boxes is not None and len(boxes) > 0:
            for box in boxes:
                cls_id = int(box.cls[0])
                conf_score = float(box.conf[0])
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                
                # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹ç»“æœï¼Œä¸ç®¡ç½®ä¿¡åº¦
                color = (0, 255, 0) if cls_id == 0 else (0, 0, 255)
                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
                label = f"{config.get('names', ['wave', 'nowave'])[cls_id]} {conf_score:.3f}"
                cv2.putText(annotated_frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                detection_count += 1
        
        # ä¿å­˜å›¾ç‰‡
        sample_path = os.path.join(output_dir, f"sample_{i+1}_frame_{frame_idx}_det_{detection_count}.jpg")
        cv2.imwrite(sample_path, annotated_frame)
        print(f"æ ·æœ¬ {i+1}: å¸§ {frame_idx}, æ£€æµ‹æ•°: {detection_count}, ä¿å­˜åˆ°: {sample_path}")
    
    cap.release()
    print(f"æ ·æœ¬å›¾ç‰‡ä¿å­˜åœ¨: {output_dir}")

def main():
    """ä¸»å‡½æ•°"""
    # ä½¿ç”¨æŒ‡å®šçš„è§†é¢‘è·¯å¾„
    video_path = r"C:\c\wave_hands\test_videos_20250622_100947\B605_3min_20250622_100957.mp4"
    
    print("=== æ¨¡å‹æ£€æµ‹èƒ½åŠ›è°ƒè¯• ===")
    print(f"è§†é¢‘æ–‡ä»¶: {video_path}")
    
    if not os.path.exists(video_path):
        print(f"é”™è¯¯ï¼šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
    except Exception as e:
        print(f"è¯»å–é…ç½®å¤±è´¥: {e}")
        return
    
    # è°ƒè¯•æ¨¡å‹æ£€æµ‹èƒ½åŠ›
    model_ok = debug_model_detection(video_path, config)
    
    # ä¿å­˜æ ·æœ¬å›¾ç‰‡ç”¨äºäººå·¥åˆ†æ
    visualize_sample_detections(video_path, config, num_samples=10)
    
    # ç»™å‡ºæœ€ç»ˆå»ºè®®
    print("\n" + "="*50)
    if not model_ok:
        print("ğŸ”´ è¯Šæ–­ç»“æœ: éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹")
        print("\nå»ºè®®æ­¥éª¤:")
        print("1. æŸ¥çœ‹ debug_samples ç›®å½•ä¸­çš„æ ·æœ¬å›¾ç‰‡")
        print("2. ç¡®è®¤è§†é¢‘ä¸­ç¡®å®æœ‰æŒ¥æ‰‹åŠ¨ä½œ")
        print("3. å¦‚æœæœ‰åŠ¨ä½œä½†æ£€æµ‹ä¸åˆ°ï¼Œåˆ™éœ€è¦é‡æ–°è®­ç»ƒ")
        print("4. æ”¶é›†å½“å‰åœºæ™¯ä¸‹çš„è®­ç»ƒæ•°æ®")
        print("5. é‡æ–°è®­ç»ƒYOLOæ¨¡å‹")
    else:
        print("ğŸŸ¢ è¯Šæ–­ç»“æœ: æ¨¡å‹å·¥ä½œæ­£å¸¸")
        print("å»ºè®®è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼æˆ–å…¶ä»–å‚æ•°")

if __name__ == '__main__':
    main() 